input {
###########################
###PROCESSSES#####
###########################	
	file {
		start_position => "beginning"
		path => "/data/process*.csv" 
		sincedb_path => "/data/sincedb/.process"
		close_older => 0		
		type => "processes"
		tags => "collection"
	}	
}
###########################
###PRE=PROCESSOR FOR PROCESSES#####
###########################	
filter {
	if [type] == "processes" {
		if [message] =~ /^HostName/ {
			drop {}
		}
	}
}
###########################
###FILTER FOR PROCESSES CSV
###########################	
filter {
	if [type] == "processes" {
		csv {
			columns => [
						"hostname",
						"host_ip",
						"process_name",
						"process_id",
						"process_path",
						"process_owner",
						"pp_name",
						"ppid",
						"module_name",
						"module_path",
						"sha1",
						"ts",
						"protocol",
						"src_ip_&_port",
						"dst_ip_&_port",
						"connection_status"
						]
		
		}
		if ["ts"] !~ /-/ {
			date { 
				match => [ "ts", "UNIX"]
				remove_field => ["ts"]
			}
		}
		mutate {
			convert => [ "process_id", "integer" ]
			convert => [ "ppid", "integer" ]
			remove_field => "message"
		}
	
	}

}
#==============================
#======OUTPUT==================
#==============================
output {
	if "collection" in [tags] {
		elasticsearch {
			index => "logstash-collections-%{+YYYY.MM.dd}"
			hosts => [ "127.0.0.1" ]
			flush_size => "10000"
			pool_max => "5000"
			pool_max_per_route => "2500"
			retry_initial_interval => "10"
			timeout => "120"
		}
	}
}
